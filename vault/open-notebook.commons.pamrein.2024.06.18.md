---
id: x239k6wncwqxfvfz9x4s3re
title: '2024-06-18'
desc: ''
updated: 1718725858963
created: 1718725661436
traitIds:
  - open-notebook-commons-pamrein
---

# PAMREIN's daily Open Notebook (COMMONS Lab)

## Todo - [Check Github](https://github.com/orgs/commons-research/projects/2/views/1)
-[]


## Meetings



## Daily report (What did I learn?)
Again I have problems with storage. I can have about 900GB memory per task. If I read in all the *.parquet files, it is not enough.
Even all the *.parquet files are only about 180GB, the compressed format will explode through the read in.  
I am looking for parallising this step, which can be maybe possible with "use_legacy_dataset=False" (https://stackoverflow.com/questions/74236493/why-reading-a-parquet-dataset-requires-much-more-memory-than-the-size-of-the-dat). 


## Future perspective



## Keywords
[[expanded_np_chemspace.abbreviations.md]]
